{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "I will be creating all vizualizations using a small sample of Fannie Mae data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../Desktop/Thesis Data/Q1_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "We use the traditional logistic regression as a base model to derive our variables of interest and to compare with our other classifiers. Let Y be our variable of interest which can take on the value of 1 or 0. Using a linear regression would result in a model that captures values greater than 1 and less than 0, which would provide no meaning and weakens the performs of the model. The logistic function's properties restrict the predictions between 1 and 0. The logistic function is seen below:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\eta) = \\frac{1}{1 + e^{-(\\eta)}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "You can see from the graphical representation that the function only outputs values between 0 and 1, and – at the mid-point – the value 0.50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We derive the logistic regression, or logit model, by applying the logistic function to the linear regression model. In the linear regression, the relationship between the response variable and the explanatory variables is modelled with the linear model:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y}^{(i)} = \\beta_{0} + \\beta_{1}x_{1}^{(i)} + ... + \\beta_{k}x_{k}^{(i)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With dichotomous classification our interest lies with the term $P(y^{(i)} = 1)$. Because with classification, probabilities should only take on values between zero and one, we plug the linear regression equation into the logistic function to get the equation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y^{(i)} = 1) = \\frac{1}{1 + e^{(-(\\beta_{0} + \\beta_{1}x_{1}^{(i)} + ... + \\beta_{k}x_{k}^{(i)}))}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Simplifying the equation to only have the linear terms on the right side of the formula gives us the popular representation of the logit model.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "log\\Bigg(\\frac{P\\big(y^{(i)} = 1\\big)}{1 - P\\big(y^{(i)} = 1\\big)}\\Bigg) = log\\Bigg(\\frac{P\\big(y^{(i)} = 1\\big)}{P\\big(y^{(i)} = 0\\big)}\\Bigg) = \\beta_{0} + \\beta_{1}x_{1}^{(i)} + ... + \\beta_{k}x_{k}^{(i)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The $log(\\eta)$ function is known as the log-odds ratio – the log of the probability of a positive outcome divided the probability of a negative outcome. Once we run the regression and obtain the weights we interpret the model by stating a one unit increase in $x_{k}$ results in a $exp(\\beta_{k})$ increase in the odds ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression, things to throw in\n",
    "(Cox, 1958) Introduction of Logistic Regress\n",
    "\n",
    "OFHEO and Frame, Gerardi, and Willen (2015)\n",
    "\n",
    "(Bagherpour)\n",
    "\"logistic regression are parametric algorithms that consider linear connections between predictors and classes. Moreover, these regressions measure the effect of changes in a predictor on the response, which is independent of the values of the other predictors.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
