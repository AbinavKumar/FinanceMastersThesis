{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanatory Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "We use the traditional logistic regression as a base model to derive our variables of interest and to compare with our other classifiers. Let Y be our variable of interest which can take on the value of 1 or 0. Using a linear regression would result in a model that captures values greater than 1 and less than 0, which would provide no meaning and weakens the performs of the model. The logistic function's properties restrict the predictions between 1 and 0. The logistic function is seen below:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\eta) = \\frac{1}{1 + e^{-(\\eta)}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "You can see from the graphical representation that the function only outputs values between 0 and 1, and – at the mid-point – the value 0.50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We derive the logistic regression, or logit model, by applying the logistic function to the linear regression model. In the linear regression, the relationship between the response variable and the explanatory variables is modelled with the linear model:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y}^{(i)} = \\beta_{0} + \\beta_{1}x_{1}^{(i)} + ... + \\beta_{k}x_{k}^{(i)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With dichotomous classification our interest lies with the term $Pr(y^{(i)} = 1)$. Because with classification, probabilities should only take on values between zero and one, we plug the linear regression equation into the logistic function to get the equation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Pr\\big(y^{(i)} = 1\\big) = \\frac{1}{1 + e^{(-(\\beta_{0} + \\beta_{1}x_{1}^{(i)} + ... + \\beta_{k}x_{k}^{(i)}))}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Simplifying the equation to only have the linear terms on the right side of the formula gives us the popular representation of the logit model.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "log\\Bigg(\\frac{Pr\\big(y^{(i)} = 1\\big)}{1 - Pr\\big(y^{(i)} = 1\\big)}\\Bigg) = log\\Bigg(\\frac{Pr\\big(y^{(i)} = 1\\big)}{Pr\\big(y^{(i)} = 0\\big)}\\Bigg) = \\beta_{0} + \\beta_{1}x_{1}^{(i)} + ... + \\beta_{k}x_{k}^{(i)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The $log(\\eta)$ function is known as the log-odds ratio – the log of the probability of a positive outcome divided the probability of a negative outcome. Once we run the regression and obtain the weights we interpret the model by stating a one unit increase in $x_{k}$ results in a $exp(\\beta_{k})$ increase in the odds ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression, things to throw in\n",
    "(Cox, 1958) Introduction of Logistic Regress\n",
    "\n",
    "OFHEO and Frame, Gerardi, and Willen (2015)\n",
    "\n",
    "(Bagherpour)\n",
    "\"logistic regression are parametric algorithms that consider linear connections between predictors and classes. Moreover, these regressions measure the effect of changes in a predictor on the response, which is independent of the values of the other predictors.\"\n",
    "\n",
    "Some stuff about it's usage or something or the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor\n",
    "K-Nearest Neighbors (KNN) is a non-parametric, supervised machine learning algorithm that seeks to find the closest K number of training examples, gauged by some distance measure. Once the K neighbors are found, the algorithm conducts a \"vote\" where the majority of nearest neighbors becomes the classification outcome. KNN is considered a \"lazy learning\" model because there is no defined training phase. The computation is offset until the classification phase. \n",
    "### FIND CITATIONS FOR EACH OF THESE TO WRITE SENTENCES?\n",
    "- Pros\n",
    "    - extremely easy to implement\n",
    "    - lazy learning, so no training, faster than training for smaller datasets\n",
    "    - new data can be easily added, because of no training\n",
    "    - only two parameters i.e. value of K and distance function\n",
    "    - K-Nearest Neighbor does not require a decision boundary compared to logistic regression, so you would expect better performance\n",
    "- Cons\n",
    "    - doesn't work well with high dimensions, because it becomes diffult to calculate\n",
    "    - KNN high prediction cost for large datasets\n",
    "    - does not work well with categorical features, because it's hard to find distance\n",
    "    - KNN not able to return coefficients or identify important predictors (low transparency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathmatical representation of KNN is as follows. Let $y^{(i)}$ be the classification of individual test observation $x^{(i)}$ while $I\\big(y^{(k)} = 1\\big)$ is an indicator function that takes the value of 1 if the i-th neighbor of the K-nearest is a positive value and 0 if it is a negative value where the nearest neighbors are determined by some distance function **(footnote papers and e.g.'s)**. In this paper we  use euclidean distance **(Make this a footnote?)**.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Pr\\big(y^{(i)} = 1 | X = x^{(i)}\\big) = \\frac{1}{K}\\sum_{k=1}^{K}I\\big(y^{(k)} = 1\\big)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(\"knn illustration.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/)\n",
    "\n",
    "Figure 1: This is an illustration for a two-dimensional classification problem, where K is 3. You can see this because the circle captures the 3 closest neighbors. In this case the 'X' marks marks the location of the test data we wish to classify while the red and blue dots symbolize their respective classes. Of the 3 closest neighbors, two of them are red, so the KNN classifier would output P(Y = 'Red' | X = x) = 2/3.\n",
    "\n",
    "Determining K is arguably the most difficult part of implementing the KNN algorithm because it determines the variance-bias trade-off of our algorithm. A small K results in low bias but high variance, because it decreases the likelihood of overfitting the data. A large K results in a high bias but a low variance, because it increases the liklihood of overfitting the data. We use a cross-validation to determine what K we ought to select for implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression Tree (CART)\n",
    "This is also referred to as the Classification and Regression Tree (CART). We will not bother using a singular decision tree in this paper, but in this explanatory section we believe it is important to let the individual decision tree have its own section to establish the basis for the explanation of the superior ensemble classifier: Random Forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree creates nodes and determines cut-off points by minimizing the classifier error rate. There are many error rate measures, but we will be using the Gini Index (James et. al, 2015). The first term in the equation below is the Gini Index, while the second term is a tuning parameter to make the tree simpler – increasing bias slightly, but significantly decreasing variance. At each node the decision tree algorithm finds the cut-off point which minimizes the function below.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G = \\sum_{k = 1}^{K}P_{i,k}(1 - P_{i,k}) + \\lambda||T||\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $P_{i,k}$ is the proportion of correctly classified observations of class $k$ in region $i$ and $\\lambda$ is a tuning parameter found through cross-validation while $||T||$ is the number of terminal nodes of the tree. For each $\\lambda$ there is a subtree $T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "The Random Forest Classifier, RFC, is an ensemble learning method **(footnote?)** that is composed of individual decision trees – each tree is created by generating a bootstrapped **(footnote?)** dataset from the training data and randomly selecting the features to split on. In the first part of this process each tree generates its own classification. We then take the average of the predictions to create a new classifier prediction, this process is call bagging (Bootstrap AGGregatING). Let N be the number of bootstraps. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f\\hat{(}x) = \\frac{1}{N}\\sum_{n=1}^{N}\\hat{f}_{n}(x)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Aggregation reduces the variance of classifiers which in turn increases the classification accuracy; however, this does not account trees that are correlated. To obtain uncorrelated trees, we use the Random Forest algorithms. The algorithm introduces randomness to the forest by evaluating all features through their error term after splitting – which in our case is the Gini Index – and by drawing random samples of the full set of features variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THINGS TO ADD TO RANDOM FOREST\n",
    "Pros\n",
    "- Really good for interpretability\n",
    "- easily handles qualitative features\n",
    "- works well with decision boundaries\n",
    "- Decorrelates \n",
    "\n",
    "Cons\n",
    "- Not as easy to interpret as a singular tree\n",
    "\n",
    "\"Random forests are a scheme proposed by Breiman (2000); Breiman (2004) to build a predictor ensemble with a set of decision trees that grows in randomly-selected subspaces of data; see Biau (2012); Geurts et al. (2006), and for a review, see Genuer et al. (2008). \" - Martey et al. Solid paper for citations..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machine\n",
    "\n",
    "Boosting is the process of transform weak learners into stronger learners; in our case we are boosting decision trees by using decision trees to decrease our error rate and increase overall performance of our model. Boosting is carried out in the following manner, where m is the interation: **Include something about gradient descent here too because you do it at the same time, maybe a visualization**\n",
    "1. Let $f_{0}(X)$ be the initial model used to predict $y$. Let $L(y_{i},\\gamma)$ be the loss function.\n",
    "$$\n",
    "\\begin{align}\n",
    "f_{0}(X) = argmin_{\\gamma}\\sum_{i=1}^{n}L(y_{i},\\gamma)\n",
    "\\end{align}\n",
    "$$\n",
    "2. Compute the gradient of the loss function. Let $\\alpha$ be the learning rate.\n",
    "$$\n",
    "\\begin{align}\n",
    "r_{i,m} = -\\alpha\\bigg[\\frac{\\partial(L(y_{i},f(x_{i}))}{\\partial f(x_{i})}\\bigg]\n",
    "\\end{align}\n",
    "$$\n",
    "3. Fit a model $h_{m}(x)$ on the gradient at each iteration.\n",
    "4. Combine $f_{m-1}(X)$ and $h_{m}(X)$ to get $f_{m}(X)$, the boosted version of $f_{m-1}(X)$. Let $\\gamma_{m}$ be the multiplier that minimizes the new loss function.\n",
    "$$\n",
    "\\begin{align}\n",
    "f_{m}(X) = f_{m-1}(X) + \\gamma h_{1}(X)\n",
    "\\end{align}\n",
    "$$\n",
    "5. Repeat the process for $M$ iterations.\n",
    "$$\n",
    "\\begin{align}\n",
    "f_{M}(X) = f_{0}(X) + \\sum_{m=1}^{M}\\gamma_{m}h_{m}(X)\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "The Support Vector Machine Algorithm (SVM) constructs a hyperplane that separates the classes. With SVM, the hyperplane's function is to separate the observations into their classes while maximizing the distance from the closest observations of two different classes, i.e. the margin.\n",
    "\n",
    "**GET A VISUALIZATION**\n",
    "\n",
    "SVM works by first systematically generating hyperplanes and then selecting the plane that maximizes the margin. Because most problems cannot be solved using a hyperplane with the same number of dimensions as features, SVM uses a kernel trick to create a higher dimensional space. By creating using a kernel, which is a function of the dot product between any two vectors of observations $(a,b)$, we significantly off-load the amount of computational effort required to increase the dimensional space.\n",
    "\n",
    "**GET A VISUALIZATION**\n",
    "\n",
    "While there are many kernel tricks we could test, for the sake of brevity we chose to test the three most popular:\n",
    "1. Linear\n",
    "$$ K(a,b) = a\\cdot b $$\n",
    "2. Polynomial\n",
    "$$ K(a,b) = (1 + a \\cdot b)^d $$\n",
    "3. Radial\n",
    "$$ K(a,b) = e^{-\\gamma(a \\cdot b)^{2}} $$\n",
    "\n",
    "We can obtain the probability estimations by fitting a logistic regresison to the decision values (Meyer et al., 2017). This is required to generate the ROC and obtain the model's AUC value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machines\n",
    "High performing in (Bagherpour et al.) introduced by (Steffen Rendle, 2010) used in this context. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
